<!DOCTYPE html>
<html lang="en"><head>  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Computer Vision Class Project
  | CS, Georgia Tech | Fall 2019: CS 6476</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

<!-- Le styles -->  
  <link href="css/bootstrap.css" rel="stylesheet">
<style>
body {
padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
}
.vis {
color: #3366CC;
}
.data {
color: #FF9900;
}
</style>
  
<link href="css/bootstrap-responsive.min.css" rel="stylesheet">

<!-- HTML5 shim, for IE6-8 support of HTML5 elements --><!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
</head>

<body>
<div class="container">
<div class="page-header">

<!-- Title and Name --> 
<h1>Human Emotion Classifier</h1> 
<span style="font-size: 20px; line-height: 1.5em;"><strong>Tarushree Gandhi: 903527176, Zayra Lobo: 903054233, Sanmeshkumar Udhayakumar: 902969263</strong></span><br>
<span style="font-size: 18px; line-height: 1.5em;">Fall 2019 CS 6476 Computer Vision: Class Project</span><br>
<span style="font-size: 18px; line-height: 1.5em;">Georgia Tech</span>
<hr>

<!-- Goal -->
<h3>Abstract</h3>
The goal of our project is to create a human emotion classifier for headshot images of people. For example, if the input is an image of a person with a sad expression, the classifier should output that the detected emotion is sadness. The motivation behind solving this problem is to provide feedback to systems that try to illicit positive human emotions, such as customer service. Our approach involves using the CK+ dataset of facial emotion images (which provides eight emotion labels based on facial expression), detecting and cropping the face from the image, extracting the facial descriptors in the images using Scale-Invariant Feature Transform (SIFT) descriptors, clustering and creating a bag of words description histogram of each image, and then training an SVM model to identify human emotions based on these bag of words description histograms. We were able to successfully classify over 70% of test images using this approach, and our primary limiting factor was likely the amount of training data we had for some emotion classes. We attempted to overcome the lack of data by augmenting the training dataset through techniques such as duplicating or flipping images to increase the size of the training data. INSERT MORE INFO ABOUT RESULTS OF DATA AUGMENTATION
<br><br>
<!-- figure -->
<h3>Teaser Output Image (from <a href="https://towardsdatascience.com/face-detection-recognition-and-emotion-detection-in-8-lines-of-code-b2ce32d4d5de">this article</a>)</h3>

<br><br>
<!-- Main Illustrative Figure --> 
<div style="text-align: center;">
<img style="height: 400px;" alt="" src="mainfig.jpeg">
</div>

<br><br>
<!-- Introduction -->
<h3>Introduction</h3>
Customer feedback is an important mechanism for businesses to continue to improve, but filling out surveys in order to provide such feedback is often perceived as a nuisance to customers. If businesses could instead detect the emotion of their customers by simply taking an image of their face while they speak with customer service, then the customers would be able to save time by not filling out a survey and the business would be able to collect more feedback. Thus, we are implementing a human emotion classifier for a headshot photograph of a person. Our classifier will use grayscale photographs as input, and we will be using the CK+ database along with machine learning techniques in order to create our classifier. We will report the overall accuracy of our classifier and the accuracy on a class-by-class basis so that we make observations such as which emotion is most difficult to classify based on this dataset.

<!--Motivation behind the problem you are solving, what applications it has, any brief background on the particular domain you are working in (if not regular RBG photographs), etc. If you are using a new way to solve an existing problem, briefly mention and describe the existing approaches and tell us how your approach is new.-->

<br><br>
<!-- Approach -->
<h3>Approach</h3>

<ol>
<li><b>CK+ Database:</b> For our experimental setup, we used the CK+ (Extended Cohn-Kanade Dataset) database, which is publicly available. It contains grayscale images of closeups of different people’s faces, with 8 different emotion labels: neutral, sadness, surprise, happiness, fear, anger, contempt and disgust. 
</li>
<!-- Original Image --> 
<div style="text-align: center;">
<img style="height: 300px;" alt="" src="originalImage.png">
</div>
<br>

<li><b>Face cropping:</b> The face is cropped from the headshot image, as that is the indicator of emotion we will be using. In order to implement face detection, we used the Haar Cascade classifier to get the face coordinates. Then, we cropped the face using these coordinates.
<br><br>
For the image preprocessing step where faces are cropped and normalized, we exploited the <a href="https://github.com/opencv/opencv/blob/master/data/haarcascades/haarcascade_frontalface_alt.xml">Haar Cascade face classifier code set</a>, which is a trained classifier for detecting objects, including faces. 

<br><br>
<!-- Cropped Image --> 
<div style="text-align: center;">
<img style="height: 300px;" alt="" src="faceCroppedImage.png">
</div>
<br>
</li>
<li><b>Facial feature extraction:</b> We extracted features from the cropped face using Scale-Invariant Feature Transform (SIFT) descriptors. In order to extract these descriptors, we exploited the OpenCV SIFT functions.

<br><br>
<!-- SIFT Keypoints Image --> 
<div style="text-align: center;">
<img style="height: 300px;" alt="" src="siftKeypointsImage.png">
</div>
<br>
</li>
<li><b>Clustering the features and BoW featurization:</b> We clustered the features using Python's scikit-learn kmeans function in order to get K-representative “visual words”. We represented this "bag of words" as a histogram of the occurrences of these words in an image. This is to represent the features of an image in a compact vector.
<br><br>
It should be noted that the histogram below only has five clusters, but the final version of our classifier will likely have many more.

<!-- Happy Histogram --> 
<div style="text-align: center;">
<img style="height: 300px;" alt="" src="histogramHappy.png">
</div>
<br>
</li>
<li><b>SVM Classification:</b> Finally, we will implement a machine learning model using Python's scikit-learn SVM classifier. We will train SVM using the histograms generated in the previous step. 70% of the dataset will used as a training set, 20% as validation set, and the rest 10% as the test set. 
</li>
<li><b>Model Evaluation on Test Dataset:</b> We will test the emotion classifier on the remaining 10% of the test dataset.</li>
</ol>

We decided to use SIFT for feature detection and BoW for featurization because an approach using these methods for human emotion classification has already been designed and used successfully, as can be seen <a href="https://towardsdatascience.com/classifying-facial-emotions-via-machine-learning-5aac111932d3#6c7a">here.</a> 
<br><br>
One obstacle we encountered while developing this approach was finding the whole set of images for each emotion, because each image for a particular emotion is in a random directory. Also, the emotion labels for each image are in a different but correlated directory from the actual image, so we had to write a script that would manually go through the directory of images and create a text file of directory paths of images for each emotion. For example, a text file called “anger.txt” would have all the image paths for angry emotion images.
<br><br>
A lack of data was also a significant obstacle that we faced with this approach. Initially, our SVM model had poor accuracy on classifying emotions with significantly less data than other emotions, so we had to resort to data augmentation to try to improve the accuracy of our model.
<!--Describe very clearly and systematically your approach to solve the problem. Tell us exactly what existing implementations you used to build your system. Tell us what obstacles you faced and how you addressed them. Justify any design choices or judgment calls you made in your approach.-->

<br><br>
<!-- Results -->
<h3>Experiment and Results</h3>
 
<h4>Proof-of-Concept Results</h4>
In order to determine whether the histograms of the descriptors in each image could indicate to machine learning models the type of emotion in that image, we ran an experiment (eight iterations with different emotions) in which we selected three sets of images from the CK+ database:
<br><br>
Set1 = 20 images of emotion 1
<br>
Set2 = 20 other images of emotion 1
<br>
Set3 = 20 images of emotion 2
<br><br>
The descriptors in the combined set of these images were clustered into 20 clusters in order to perform dimension reduction and keep the bag of words description of each image small. Then a histogram was created for each image of the total number of each "word" present in that image.
<br><br>
Then we measured the correlation between the following using the numpy corrcoef() function:
<br><br>
<b>Set1 and Set2 (Set1_2 correlation):</b> We expect this to be a high correlation close to 1, cause the correlation is between images of the same emotion.
<br>
<b>Set1 and Set3 (Set1_3 correlation):</b> We expect this to be a correlation lower than that of Set1_2, since Set1_3 is comparing sets of different emotions.
<br>
<b>Set2 and Set3 (Set2_3 correlation):</b> We expect this to be a correlation lower than that of Set1_2, since Set2_3 is comparing sets of different emotions.
<br><br>
Below are the correlation results for the two happy and one sadness data sets. The correlation was as expected, with the two sets of happy data sets being highly correlated while the correlation of either happy set with the sadness set was much lower. Examples of the input images with SIFT keypoints drawn on them and the histograms used for correlation can be found in the Qualitative Results section.
<br><br>
<b>Happy (Set 1) vs. Happy (Set 2) correlation:</b> 0.81 (highest correlation, close to 1)
<br>
<b>Happy (Set 1) vs. Sadness (Set 3) correlation:</b> 0.49 (lower than Set1_2)
<br>
<b>Happy (Set 2) vs. Sadness (Set 3) correlation:</b> 0.38 (lower than Set1_2)
<br><br>
Correlations were taken between some other random combinations of emotions as well, and in almost every case, the correlations were as expected. These results indicate that the histograms of the descriptors in each image can determine the type of emotion in an image.
<br><br>
Note that in the correlation table below, every set of correlation in each row follows the title format of "Set1_2 correlation", "Set1_3 correlation", and "Set2_3 correlation". Also note that the correlation of happy1happy2 in the happy vs. sadness column will be different than the happy1happy2  correlation in the happy vs. disgust column, because the data was clustered differently in each of the experiments.

<br><br>
<b>Observations:</b>
<ol type="1">
<li>We noticed as k clusters and number of images were increased, the correlation behaved in a more expected way. This makes sense because the more data there is available and the more clusters there are, the more accurately the descriptors can become labelled as features, and can represent the images better.</li>
<li>The emotions that had the least correlation difference were emotions that were similar. For example, the disgust correlation in red between disgust and anger doesn’t behave as expected since it is bigger than the correlation between the two sets of disgust images. This is explainable because disgust and anger are both negative emotions that can be similar. Meanwhile, disgust and surprise are more distinct emotions with 1 being negative and 1 being positive, and this is indicated by the large difference in correlation.</li>
</ol>

The table below displays the results for all of the correlation experiments we ran as a proof-of-concept. Each experiment compares 3 datasets of 20 images each. The largest correlation is colored in each experiment. This correlation is blue if it is the correlation that is expected to be the largest (correlation between sets of images of the same emotion) and red if this is not the case.

<br><br>
<center>
<table style="width:100%">
  <table border="1">
  <tr>
    <th>Exp no.</th>
    <th>Emotions</th> 
    <th>Correlation</th>
    <th>Emotions</th> 
    <th>Correlation</th>
    <th>Emotions</th> 
    <th>Correlation</th>
  </tr>
  <tr>
    <td>Exp 1</td>
    <td>Happy1,Happy2</td>
    <td><font color="blue">0.81</font></td> 
    <td>Happy1,Sad1</td>
    <td>0.49</td>
    <td>Happy2,Sad1</td>
    <td>0.38</td>
  </tr>
  <tr>
    <td>Exp 2</td>
    <td>Happy1,Happy2</td>
    <td><font color="blue">0.80</font></td> 
    <td>Happy1,Disgust3</td>
    <td>0.79</td>
    <td>Happy2,Disgust3</td>
    <td>0.55</td>
  </tr>
  <tr>
    <td>Exp 3</td>
    <td>Happy1,Happy2</td>
    <td><font color="blue">0.86</font></td> 
    <td>Happy1,Anger3</td>
    <td>0.79</td>
    <td>Happy2,Anger</td>
    <td>0.79</td>
  </tr>
  <tr>
    <td>Exp 4</td>
    <td>Happy1,Happy2</td>
    <td><font color="blue">0.88</font></td> 
    <td>Happy1,Surprise3</td>
    <td>0.54</td>
    <td>Happy2,Surprise3</td>
    <td>0.32</td>
  </tr>
  
  <tr>
    <td>Exp 5</td>
    <td>Disgust1,Disgust2</td>
    <td>0.82</td> 
    <td>Disgust1,Anger3</td>
    <td><font color="red">0.84</font></td>
    <td>Disgust2,Anger3</td>
    <td>0.81</td>
  </tr>
  
  <tr>
    <td>Exp 6</td>
    <td>Disgust1,Disgust2</td>
    <td><font color="blue">0.76</font></td> 
    <td>Disgust1,Surprise3</td>
    <td>0.64</td>
    <td>Disgust2,Surprise3</td>
    <td>0.39</td>
  </tr>
  
  <tr>
    <td>Exp 7</td>
    <td>Disgust1,Disgust2</td>
    <td><font color="blue">0.75</font></td> 
    <td>Disgust1,Fear3</td>
    <td>0.74</td>
    <td>Disgust2,Fear3</td>
    <td>0.57</td>
  </tr>
  
   <tr>
    <td>Exp 8</td>
    <td>Disgust1,Disgust2</td>
    <td><font color="blue">0.84</font></td> 
    <td>Disgust1,Happy3</td>
    <td>0.78</td>
    <td>Disgust2,Happy3</td>
    <td>0.72</td>
  </tr>
  
</table>
</center>
<br><br>

<h4>Final Experiment Results</h4>
For our final experiment, we first calculated the BoW histograms for each image in our dataset. Next, we used 80% of the data to train an SVM model and then used the model to predict the labels of the remaining 20% of the data. We defined success for this experiment as correctly identifying 50% of these images.

<h5><u>Test 1: k = 500, 80/20 training/test data, no data augmentation</u></h5>
We started out with 500 clusters for our BoW clustering. We then trained our SVM model on those BoW vectors and tested it on our remaining data. Unfortunately, not all of the emotion classes had the same amount of data available for us to train our model on. Our initial distribution of training and test data for each class was as follows:

<br><br>
<center>
<table style="width:100%">
  <table border="1">
  <tr>
    <th>Emotion</th>
    <th>Number of Training Images</th>
    <th>Number of Test Images</th>
    <th>Total Number of Images</th>
  </tr>
  <tr>
    <td>Anger</td>
    <td>36</td>
    <td>9</td>
    <td>45</td>
  </tr>
  <tr>
    <td>Contempt</td>
    <td>14</td>
    <td>4</td>
    <td>18</td>
  </tr>
  <tr>
    <td>Disgust</td>
    <td>47</td>
    <td>12</td>
    <td>59</td>
  </tr>
  <tr>
    <td>Fear</td>
    <td>20</td>
    <td>5</td>
    <td>25</td>
  </tr>
  <tr>
    <td>Happy</td>
    <td>55</td>
    <td>14</td>
    <td>69</td>
  </tr>
  <tr>
    <td>Sadness</td>
    <td>22</td>
    <td>6</td>
    <td>28</td>
  </tr>
  <tr>
    <td>Surprise</td>
    <td>65</td>
    <td>17</td>
    <td>82</td>
  </tr>
</table>
</center>

<br>Due to the low number of samples for anger, contempt, fear, and sadness, we only achieved an overall accuracy of <b>56.72%.</b> The confusion matrix of our SVM classification was as follows:<br><br>

<center>
<table style="width:100%">
  <table border="1">
  <tr>
    <th colspan="2" rowspan="2"></th>
    <th colspan="7">Predicted Emotion</th>
  </tr>
  <tr>
    <th>Anger</th>
    <th>Contempt</th>
    <th>Disgust</th>
    <th>Fear</th>
    <th>Happy</th>
    <th>Sadness</th>
    <th>Surprise</th>
  </tr>
  <tr>
    <th rowspan="9">Actual Emotion</th>
    <th>Anger</td>
    <td>1</td>
    <td>0</td>
    <td>1</td>
    <td>0</td>
    <td>1</td>
    <td>0</td>
    <td>6</td>
  </tr>
  <tr>
    <th>Contempt</td>
    <td>0</td>
    <td>0</td>
    <td>1</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>3</td>
  </tr>
  <tr>
    <th>Disgust</td>
    <td>0</td>
    <td>0</td>
    <td>11</td>
    <td>0</td>
    <td>1</td>
    <td>0</td>
    <td>0</td>
  </tr>
  <tr>
    <th>Fear</td>
    <td>1</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>1</td>
    <td>0</td>
    <td>3</td>
  </tr>
  <tr>
    <th>Happy</td>
    <td>0</td>
    <td>0</td>
    <td>3</td>
    <td>0</td>
    <td>11</td>
    <td>0</td>
    <td>0</td>
  </tr>
  <tr>
    <th>Sadness</td>
    <td>0</td>
    <td>0</td>
    <td>1</td>
    <td>0</td>
    <td>1</td>
    <td>0</td>
    <td>4</td>
  </tr>
  <tr>
    <th>Surprise</td>
    <td>1</td>
    <td>0</td>
    <td>1</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>15</td>
  </tr>
</table>
</center>

<br><br>

Although our overall accuracy surpassed the 50% threshold that we defined as our success level, we wanted to see if we could improve the accuracy of our classifier. First, we tried to increase the number of clusters for our BoW vectors.

<h5><u>Test 2: k = 1000, 80/20 training/test data, no data augmentation</u></h5>

We hypothesized that increasing the number of clusters for our SIFT descriptors from 500 to 1000 would make our BoW vectors more detailed and thus more distinct for each emotion. However, we ended up achieving poorer accuracy, with an overall accuracy of <b>52.24%</b>. This decrease in accuracy, in spite of a supposed increase in the detail of the BoWs, could have been due to a lack of distinct SIFT descriptors for each emotion. If each emotion already did not have many descriptors specific to that class (perhaps due to a lack of training data), then increasing the number of clusters would not help increase the accuracy of classification. Below is the confusion matrix for 1000 clusters.

<center>
<br><br>
<table style="width:100%">
  <table border="1">
  <tr>
    <th colspan="2" rowspan="2"></th>
    <th colspan="7">Predicted Emotion</th>
  </tr>
  <tr>
    <th>Anger</th>
    <th>Contempt</th>
    <th>Disgust</th>
    <th>Fear</th>
    <th>Happy</th>
    <th>Sadness</th>
    <th>Surprise</th>
  </tr>
  <tr>
    <th rowspan="9">Actual Emotion</th>
    <th>Anger</td>
    <td>0</td>
    <td>0</td>
    <td>1</td>
    <td>0</td>
    <td>2</td>
    <td>0</td>
    <td>6</td>
  </tr>
  <tr>
    <th>Contempt</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>4</td>
  </tr>
  <tr>
    <th>Disgust</td>
    <td>0</td>
    <td>0</td>
    <td>5</td>
    <td>0</td>
    <td>4</td>
    <td>0</td>
    <td>3</td>
  </tr>
  <tr>
    <th>Fear</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>1</td>
    <td>0</td>
    <td>4</td>
  </tr>
  <tr>
    <th>Happy</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>14</td>
    <td>0</td>
    <td>0</td>
  </tr>
  <tr>
    <th>Sadness</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>1</td>
    <td>0</td>
    <td>5</td>
  </tr>
  <tr>
    <th>Surprise</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>1</td>
    <td>0</td>
    <td>16</td>
  </tr>
</table>
</center>
<br><br>

Because increasing the number of clusters did not improve our overall classification accuracy, we returned to using 500 clusters and tried to modify how we split our training and test data for the emotions with fewer samples.

<h5><u>Test 3: k = 500, ~90/10 training/test data (for emotions with fewer samples), no data augmentation</u></h5>

Because we seemed to be getting a lower overall accuracy due to some classes have less data to train the SVM model on, we tried moving all but a few samples from the test data for these classes to the training data. Below is the new breakdown of how we decided to split the data for these emotion classes, with the bolded rows being the emotions that changed from an 80/20 training/test split to closer to 90/10:

<br><br>
<center>
<table style="width:100%">
  <table border="1">
  <tr>
    <th>Emotion</th>
    <th>Number of Training Images</th>
    <th>Number of Test Images</th>
    <th>Total Number of Images</th>
  </tr>
  <tr>
    <td><b>Anger</b></td>
    <td><b>42</b></td>
    <td><b>3</b></td>
    <td><b>45</b></td>
  </tr>
  <tr>
    <td><b>Contempt</b></td>
    <td><b>16</b></td>
    <td><b>2</b></td>
    <td><b>18</b></td>
  </tr>
  <tr>
    <td>Disgust</td>
    <td>47</td>
    <td>12</td>
    <td>59</td>
  </tr>
  <tr>
    <td><b>Fear</b></td>
    <td><b>23</b></td>
    <td><b>2</b></td>
    <td><b>25</b></td>
  </tr>
  <tr>
    <td>Happy</td>
    <td>55</td>
    <td>14</td>
    <td>69</td>
  </tr>
  <tr>
    <td><b>Sadness</b></td>
    <td><b>26</b></td>
    <td><b>2</b></td>
    <td><b>28</b></td>
  </tr>
  <tr>
    <td>Surprise</td>
    <td>65</td>
    <td>17</td>
    <td>82</td>
  </tr>
</table>
</center>
<br>

Using this new training/test split for emotions with less data available, we observed an increase in overall accuracy to <b>71.15%.</b> Below is the confusion matrix corresponding with this new split of training and test data:

<center>
<br>
<table style="width:100%">
  <table border="1">
  <tr>
    <th colspan="2" rowspan="2"></th>
    <th colspan="7">Predicted Emotion</th>
  </tr>
  <tr>
    <th>Anger</th>
    <th>Contempt</th>
    <th>Disgust</th>
    <th>Fear</th>
    <th>Happy</th>
    <th>Sadness</th>
    <th>Surprise</th>
  </tr>
  <tr>
    <th rowspan="9">Actual Emotion</th>
    <th>Anger</td>
    <td>1</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>1</td>
    <td>1</td>
  </tr>
  <tr>
    <th>Contempt</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>2</td>
  </tr>
  <tr>
    <th>Disgust</td>
    <td>1</td>
    <td>0</td>
    <td>10</td>
    <td>0</td>
    <td>1</td>
    <td>0</td>
    <td>0</td>
  </tr>
  <tr>
    <th>Fear</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>1</td>
    <td>0</td>
    <td>1</td>
  </tr>
  <tr>
    <th>Happy</td>
    <td>0</td>
    <td>0</td>
    <td>3</td>
    <td>0</td>
    <td>11</td>
    <td>0</td>
    <td>0</td>
  </tr>
  <tr>
    <th>Sadness</td>
    <td>1</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>1</td>
  </tr>
  <tr>
    <th>Surprise</td>
    <td>2</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>15</td>
  </tr>
</table>
</center>
<br>

Though we observed an increase in the overall accuracy of our classifier, we knew that we were still not training or testing on many samples for the emotions with less data available. So, we next tried to increase the amount of training data we had available using data augmentation.

<h5><u>Test 4: k = 500, 80/20 training/test data, data augmentation: repeated training images</u></h5>

The first type of data augmentation that we tried was repeating the same training data multiple times in the training set till we got a number of samples similar to the number in the other classes. Prior to this repetition of data, some of the training data was moved back to the test set so that we could have an 80/20 split of the data for training/testing again. Below is the result of the data augmentation on the number of samples in each class (bolded rows are the emotions for which we augmented the data):

<br><br>
<center>
<table style="width:100%">
  <table border="1">
  <tr>
    <th>Emotion</th>
    <th>Number of Training Images</th>
    <th>Number of Test Images</th>
    <th>Total Number of Images</th>
  </tr>
  <tr>
    <td><b>Anger</b></td>
    <td><b>44</b></td>
    <td><b>11</b></td>
    <td><b>55</b></td>
  </tr>
  <tr>
    <td><b>Contempt</b></td>
    <td><b>44</b></td>
    <td><b>11</b></td>
    <td><b>55</b></td>
  </tr>
  <tr>
    <td>Disgust</td>
    <td>47</td>
    <td>12</td>
    <td>59</td>
  </tr>
  <tr>
    <td><b>Fear</b></td>
    <td><b>44</b></td>
    <td><b>11</b></td>
    <td><b>55</b></td>
  </tr>
  <tr>
    <td>Happy</td>
    <td>55</td>
    <td>14</td>
    <td>69</td>
  </tr>
  <tr>
    <td><b>Sadness</b></td>
    <td><b>44</b></td>
    <td><b>11</b></td>
    <td><b>55</b></td>
  </tr>
  <tr>
    <td>Surprise</td>
    <td>65</td>
    <td>17</td>
    <td>82</td>
  </tr>
</table>
</center>
<br>

Using this new training/test data, we observed a decrease in overall accuracy to <b>45.98%.</b> Below is the confusion matrix corresponding with this new split of training and test data:

<center>
<br>
<table style="width:100%">
  <table border="1">
  <tr>
    <th colspan="2" rowspan="2"></th>
    <th colspan="7">Predicted Emotion</th>
  </tr>
  <tr>
    <th>Anger</th>
    <th>Contempt</th>
    <th>Disgust</th>
    <th>Fear</th>
    <th>Happy</th>
    <th>Sadness</th>
    <th>Surprise</th>
  </tr>
  <tr>
    <th rowspan="9">Actual Emotion</th>
    <th>Anger</td>
    <td>3</td>
    <td>0</td>
    <td>4</td>
    <td>0</td>
    <td>0</td>
    <td>1</td>
    <td>3</td>
  </tr>
  <tr>
    <th>Contempt</td>
    <td>1</td>
    <td>0</td>
    <td>2</td>
    <td>2</td>
    <td>1</td>
    <td>0</td>
    <td>5</td>
  </tr>
  <tr>
    <th>Disgust</td>
    <td>0</td>
    <td>0</td>
    <td>11</td>
    <td>0</td>
    <td>1</td>
    <td>0</td>
    <td>0</td>
  </tr>
  <tr>
    <th>Fear</td>
    <td>1</td>
    <td>0</td>
    <td>1</td>
    <td>0</td>
    <td>4</td>
    <td>0</td>
    <td>5</td>
  </tr>
  <tr>
    <th>Happy</td>
    <td>0</td>
    <td>0</td>
    <td>3</td>
    <td>0</td>
    <td>11</td>
    <td>0</td>
    <td>0</td>
  </tr>
  <tr>
    <th>Sadness</td>
    <td>3</td>
    <td>1</td>
    <td>2</td>
    <td>1</td>
    <td>1</td>
    <td>0</td>
    <td>3</td>
  </tr>
  <tr>
    <th>Surprise</td>
    <td>2</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>15</td>
  </tr>
</table>
</center>
<br>

This significant decrease in accuracy could have been due to a lack of variety in the training data, in spite of the fact that we provided the model with more of it. If the data does not provide new information about the BoW vectors for that emotion, it seems as though the classification will continue to use poor metrics for classifiying different emotions. In fact, the classification may be even poorer because the training data is providing the same BoW vectors repeatedly, causing the classifier to believe that most BoW vectors in that class will have the exact same pattern, just as the data repeats the same vectors again and again. To attempt to alleviate this issue, we tried augmenting the data in a different way.

<!--Provide details about the experimental set up (number of images/videos, number of datasets you experimented with, train/test split if you used machine learning algorithms, etc.). Describe the evaluation metrics you used to evaluate how well your approach is working. Include clear figures and tables, as well as illustrative qualitative examples if appropriate. Be sure to include obvious baselines to see if your approach is doing better than a naive approach (e.g. for classification accuracy, how well would a classifier do that made random decisions?). Also discuss any parameters of your algorithms, and tell us how you set the values of those parameters. You can also show us how the performance varies as you change those parameter values. Be sure to discuss any trends you see in your results, and explain why these trends make sense. Are the results as expected? Why?-->

<h3>Qualitative Results</h3>

<br><br>
<h4><center>SIFT Descriptor Examples</h4></center>
<div style="text-align: center;">
<img style="height: 300px;" alt="" src="happyFaceKeypoints.png">
<img style="height: 300px;" alt="" src="sadnessFaceKeypoints.png">
</div>

<br><br>
<h4><center>Example of Histograms of Descriptor Clusters (Bag of Words Vectors)</h4></center>
<div style="text-align: center;">
<img style="height: 300px;" alt="" src="happyHistogramExampleFinal.png">
<img style="height: 300px;" alt="" src="sadHistogramExampleFinal.png">
</div>

<h3>Conclusion and Future Work</h3>
<!--Conclusion would likely make the same points as the abstract. Discuss any future ideas you have to make your approach better.-->
We demonstrated that human emotion classifcation can be performed with greater than 50% accuracy using BoW vectorization on images to train an SVM model. As is common in machine learning applications, the greatest obstacle faced with this technique was the lack of training data for our SVM model. Data augmentation methods can be helpful with this INSERT MORE INFO ABOUT DATA AUGMENTATION RESULTS
<br><br>
For future work, we would recommend gathering more training data for the SVM model in order to get more accurate results for the anger, contempt, fear, and sadness classes. Also, looking into other methods of grouping the SIFT descriptors, such as VLAD (Vector of Locally Aggregated Descriptors), could yield more accurate classification results than BoW did.

<br><br>

<h3>References</h3>
<a href="https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_objdetect/py_face_detection/py_face_detection.html">OpenCV Tutorial on Face Detection using Haar Cascades</a>
<br>
<a href="https://towardsdatascience.com/classifying-facial-emotions-via-machine-learning-5aac111932d3#6c7a">Article about Classifying Human Emotions using Machine Learning</a>
<br>
<a href="https://docs.opencv.org/3.4/da/df5/tutorial_py_sift_intro.html">OpenCV Tutorial on SIFT</a>

<br><br>

<a href="midterm_update.html"><h3>Midterm Update</a></h3>

<br>

<a href="project_proposal.html"><h3>Project Proposal</a></h3>

<br><br>
  <hr>
  <footer> 
  <p>© Gandhi, Lobo, Udhayakumar</p>
  </footer>
</div>
</div>
<br><br>

</body></html>